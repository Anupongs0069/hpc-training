### üöÄ **‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 7: ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU ‡∏ö‡∏ô LANTA HPC** üöÄ  

---

## üìå **‡∏ö‡∏ó‡∏ô‡∏≥**  
**LANTA HPC** ‡πÄ‡∏õ‡πá‡∏ô‡∏ã‡∏π‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏û‡∏•‡∏±‡∏á‡∏Ç‡∏≠‡∏á GPU ‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå (AI), Deep Learning, ‡πÅ‡∏•‡∏∞ Machine Learning ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU ‡∏ö‡∏ô LANTA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

---

## üéØ **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô**
1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á HPC ‡πÅ‡∏•‡∏∞ GPU ‡∏ö‡∏ô LANTA  
2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô **PyTorch** ‡∏ö‡∏ô GPU  
3. ‡∏ù‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏ß‡∏¢ **SLURM Scheduler**  
4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡πÇ‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á HPC  
5. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Multi-GPU  

---

## üñ• **1. ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà LANTA HPC**
‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö LANTA ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ SSH

1. **Windows:** ‡πÉ‡∏ä‡πâ **MobaXterm** ‡∏´‡∏£‡∏∑‡∏≠ **PuTTY**
2. **macOS/Linux:** ‡πÉ‡∏ä‡πâ Terminal ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:
   ```bash
   ssh [‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ]@lanta.nstda.or.th
   ```

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà **‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô** ‡πÅ‡∏•‡∏∞ **Two-Factor Authentication (2FA)**

---

## üìÇ **2. ‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡πÇ‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á LANTA**
### **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ SFTP ‡πÉ‡∏ô MobaXterm (Windows)**
1. ‡πÄ‡∏õ‡∏¥‡∏î **MobaXterm**
2. ‡∏Å‡∏î `Session` ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å `SFTP`  
3. ‡πÉ‡∏™‡πà `Remote Host`: `transfer.lanta.nstda.or.th`  
4. ‡∏Å‡∏£‡∏≠‡∏Å `Username` ‡πÅ‡∏•‡∏∞ `Password`  
5. ‡πÉ‡∏™‡πà **2FA Code** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô  
6. ‡∏•‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏ö‡∏ô LANTA  

### **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ SCP ‡∏ö‡∏ô macOS/Linux**
**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå:**  
```bash
scp myfile.py [‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ]@transfer.lanta.nstda.or.th:/home/[‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ]/
```
**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå:**  
```bash
scp -r myfolder/ [‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ]@transfer.lanta.nstda.or.th:/home/[‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ]/
```

---

## üöÄ **3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô PyTorch ‡∏ö‡∏ô GPU**
### **‡∏™‡∏£‡πâ‡∏≤‡∏á Virtual Environment ‡∏ö‡∏ô LANTA**
1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡∏î‡∏π‡∏• **Mamba**:
   ```bash
   module load Mamba/23.11.0-0
   ```
2. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô environment:
   
   ```bash
   conda activate pytorch-2.2.2
   ```
---

## üß† **3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ GPU**
‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Python ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ GPU ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ:
```python
import torch

print("Torch version:", torch.__version__)
print("CUDA Available:", torch.cuda.is_available())
print("CUDA Version:", torch.version.cuda)
print("GPU Count:", torch.cuda.device_count())
```

‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏™‡∏î‡∏á `CUDA Available: True` ‡πÅ‡∏•‡∏∞ `GPU Count` ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0 ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ HPC ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö GPU ‚úÖ

---

## üìå **4. ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Multi-GPU**
## ‡πÄ‡∏•‡πà‡∏ô Tensorflow Playground
https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.48315&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false

### **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î PyTorch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multi-GPU**
```python
import torch
import torch.nn as nn

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô GPU ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
gpu_count = torch.cuda.device_count()

print(f"‡πÉ‡∏ä‡πâ {gpu_count} GPUs!")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# ‡πÉ‡∏ä‡πâ DataParallel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏•‡∏≤‡∏¢ GPU
model = SimpleModel()
if gpu_count > 1:
    model = nn.DataParallel(model)

model.to(device)

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
x = torch.randn(5, 10).to(device)
output = model(x)
print(output)
```

---

## üìú **5. ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á LANTA GPU Node**
LANTA ‡πÉ‡∏ä‡πâ **SLURM Scheduler** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `job_gpu.slurm` ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ:
```bash
#!/bin/bash
#SBATCH -p gpu  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU Node
#SBATCH -N 1     # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏´‡∏ô‡∏î
#SBATCH -c 16    # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU Core ‡∏ï‡πà‡∏≠‡πÇ‡∏´‡∏ô‡∏î
#SBATCH --gpus-per-task=1  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô GPU ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
#SBATCH --ntasks-per-node=4  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÇ‡∏´‡∏ô‡∏î
#SBATCH -t 02:00:00  # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á:‡∏ô‡∏≤‡∏ó‡∏µ:‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
#SBATCH -A cb9009xx  # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
#SBATCH -J my_gpu_job  # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ job

module load Mamba/23.11.0-0
conda activate pytorch-2.2.2
export PATH=/lustrefs/disk/modules/easybuild/software/Mamba/23.11.0-0/envs/pytorch-2.2.2/bin:$PATH
python3 multi-gpu.py
```
‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á **sbatch** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô:
```bash
sbatch job_gpu.slurm
```

---

## üéØ **7. ‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô**
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô:
```bash
myqueue
```
‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π log file:
```bash
cat slurm-[JOB_ID].out
```
‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô:
```bash
scancel [JOB_ID]
```

---

## üì¢ **8. ‡∏™‡∏£‡∏∏‡∏õ**
‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà HPC ‡∏î‡πâ‡∏ß‡∏¢ SSH  
‚úÖ ‡∏ñ‡πà‡∏≤‡∏¢‡πÇ‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ SFTP ‡∏´‡∏£‡∏∑‡∏≠ SCP  
‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Virtual Environment ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch  
‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU  
‚úÖ ‡πÉ‡∏ä‡πâ Multi-GPU ‡∏î‡πâ‡∏ß‡∏¢ PyTorch  
‚úÖ ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡πà‡∏≤‡∏ô SLURM  

---

üí° **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:**  
- HPC LANTA ‡∏°‡∏µ **NVIDIA A100 GPUs** ‡∏ó‡∏µ‡πà‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û  
- ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö **‡πÇ‡∏°‡∏î‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à** ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ `module list` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö  
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà: [ThaiSC LANTA](https://thaisc.io/th/thaisc-resources/lanta)  

üìå **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á**:  
- ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ Multi-GPU Training  
- ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á pip ‡πÅ‡∏•‡∏∞ conda ‡∏ö‡∏ô LANTA  
- ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡πÇ‡∏≠‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ô LANTA  

---

